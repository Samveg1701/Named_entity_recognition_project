{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVEF8x2Z55OE"
      },
      "source": [
        "### POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bVp80mVe1j4-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score,confusion_matrix\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "import spacy\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "# from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install plotly\n",
        "# !pip install pandas\n",
        "# !pip install spacy\n",
        "# !pip install tqdm\n",
        "!pip install matplotlib, seaborn, collections, ast, sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZEbIgKXPXmP",
        "outputId": "a4312d57-c0e2-4dc8-e093-180ab1b73156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EBqDcppX1l18",
        "outputId": "54e25c89-3227-489c-e1e3-699440a3a128"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0       category  \\\n",
              "0           0           tech   \n",
              "1           1       business   \n",
              "2           2          sport   \n",
              "3           3          sport   \n",
              "4           4  entertainment   \n",
              "\n",
              "                                                text  \n",
              "0  tv future in the hands of viewers with home th...  \n",
              "1  worldcom boss  left books alone  former worldc...  \n",
              "2  tigers wary of farrell  gamble  leicester say ...  \n",
              "3  yeading face newcastle in fa cup premiership s...  \n",
              "4  ocean s twelve raids box office ocean s twelve...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"C:/Users/asus/OneDrive/Desktop/NLP assignment/NLP_task_2.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQI_2-OCphUL",
        "outputId": "68e40ced-fd2d-47f7-8ce1-699719c43fbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/30 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<unknown>, line 1)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3508\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[5], line 8\u001b[0m\n    words = ast.literal_eval(df.text.iloc[i])\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:64\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
            "\u001b[1;36m  File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
            "\u001b[1;36m  File \u001b[1;32m<unknown>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Load a spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"lemmatizer\"])\n",
        "\n",
        "pos_list = []\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "    # Convert string representation of list back to list\n",
        "    words = ast.literal_eval(df.text.iloc[i])\n",
        "\n",
        "    # Use spaCy to create a Doc from the list of words (tokens)\n",
        "    doc = spacy.tokens.Doc(nlp.vocab, words=words)\n",
        "\n",
        "    # Apply the pipeline's components except tokenizer (since we already have tokens)\n",
        "    for name, proc in nlp.pipeline:\n",
        "        doc = proc(doc)\n",
        "\n",
        "    # Extract POS tags\n",
        "    pos_tags = [token.tag_ for token in doc]\n",
        "    pos_list.append(pos_tags)\n",
        "\n",
        "# Update DataFrame with the new columns\n",
        "df['predicted_POS'] = pos_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/30 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<unknown>, line 1)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3508\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[7], line 28\u001b[0m\n    pos_tags = process_text(df.text.iloc[i])\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[7], line 11\u001b[0m in \u001b[0;35mprocess_text\u001b[0m\n    words = ast.literal_eval(text)\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:64\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
            "\u001b[1;36m  File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
            "\u001b[1;36m  File \u001b[1;32m<unknown>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JERjZyzLkLBZ"
      },
      "outputs": [],
      "source": [
        "def parse_column(entry):\n",
        "    # Split the entry to get the list elements\n",
        "    elements = entry.split(']')[0].split('[')[1].split(', ')\n",
        "    # Strip single and double quotes from each element\n",
        "    stripped_elements = [element.strip(\"'\\\"\") for element in elements]\n",
        "    return stripped_elements\n",
        "\n",
        "df['Word'] = df['Word'].apply(lambda x: parse_column(x))\n",
        "df['POS'] = df['POS'].apply(lambda x: parse_column(x))\n",
        "df['Tag'] = df['Tag'].apply(lambda x: parse_column(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7AHSne7jouC"
      },
      "outputs": [],
      "source": [
        "pos = df['POS'].astype('str')\n",
        "predicted_pos = df['predicted_POS'].astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdpj4n6tjcJE"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(pos, predicted_pos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoI9y1WgmmoG"
      },
      "outputs": [],
      "source": [
        "# Flatten the lists\n",
        "y_true_pos = [tag for sublist in df['POS'] for tag in sublist]\n",
        "y_pred_pos = [tag for sublist in df['predicted_POS'] for tag in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6WI5emSnNDE"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(y_true_pos,y_pred_pos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu1kwKYuphG"
      },
      "outputs": [],
      "source": [
        "# Identify all unique labels\n",
        "all_labels = sorted(set(y_true_pos + y_pred_pos))\n",
        "\n",
        "# Count the support for each label in y_true\n",
        "support_counts = {label: y_true.count(label) for label in all_labels}\n",
        "\n",
        "# Filter out labels with zero support\n",
        "labels_with_support = [label for label, count in support_counts.items() if count > 0]\n",
        "\n",
        "# Filter y_true and y_pred to include only instances with labels having non-zero support\n",
        "y_true_pos_filtered = [y_true[i] for i in range(len(y_true_pos)) if y_true[i] in labels_with_support]\n",
        "y_pred_pos_filtered = [y_pred[i] for i in range(len(y_true_pos)) if y_true[i] in labels_with_support]\n",
        "\n",
        "# Generate the classification report for filtered lists\n",
        "report = classification_report(y_true_pos_filtered, y_pred_pos_filtered, labels=labels_with_support, zero_division=0)\n",
        "\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZk8HKYhylGi"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_pos_filtered, y_pred_pos_filtered, labels=labels_with_support)\n",
        "\n",
        "# Extract labels from the data\n",
        "labels = labels_with_support\n",
        "\n",
        "# Create a heatmap using Plotly Graph Objects\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=cm,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Confusion Matrix',\n",
        "    xaxis_title='Predicted Label',\n",
        "    yaxis_title='True Label',\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(labels))),\n",
        "        ticktext=labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(labels))),\n",
        "        ticktext=labels,\n",
        "        tickangle=-45  # Rotate labels\n",
        "    ),\n",
        "    autosize=False,\n",
        "    width=800,  # Adjust width\n",
        "    height=800,  # Adjust height to accommodate rotated labels\n",
        "    margin=dict(l=200, r=50, t=50, b=100)  # Adjust margins (left, right, top, bottom)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DI7Kz1E1LQv"
      },
      "source": [
        "### NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qaYk9nAIwtBr"
      },
      "outputs": [],
      "source": [
        "tag_map = {\n",
        "  \"B-PERSON\": \"B-per\", \"I-PERSON\": \"I-per\",\n",
        "  \"B-NORP\": \"B-org\", \"I-NORP\": \"I-org\",\n",
        "  \"B-FAC\": \"B-geo\", \"I-FAC\": \"I-geo\",\n",
        "  \"B-ORG\": \"B-org\", \"I-ORG\": \"I-org\",\n",
        "  \"B-GPE\": \"B-gpe\", \"I-GPE\": \"I-gpe\",\n",
        "  \"B-LOC\": \"B-geo\", \"I-LOC\": \"I-geo\",\n",
        "  \"B-PRODUCT\": \"B-art\", \"I-PRODUCT\": \"I-art\",\n",
        "  \"B-EVENT\": \"B-eve\", \"I-EVENT\": \"I-eve\",\n",
        "  \"B-WORK_OF_ART\": \"B-art\", \"I-WORK_OF_ART\": \"I-art\",\n",
        "  \"B-LAW\": \"B-art\", \"I-LAW\": \"I-art\",\n",
        "  \"B-LANGUAGE\": \"B-org\", \"I-LANGUAGE\": \"I-org\",\n",
        "  \"B-DATE\": \"B-tim\", \"I-DATE\": \"I-tim\",\n",
        "  \"B-TIME\": \"B-tim\", \"I-TIME\": \"I-tim\",\n",
        "  \"B-QUANTITY\": \"B-num\", \"I-QUANTITY\": \"I-num\",\n",
        "  \"B-ORDINAL\": \"B-num\", \"I-ORDINAL\": \"I-num\",\n",
        "  \"B-CARDINAL\": \"B-num\", \"I-CARDINAL\": \"I-num\",\n",
        "  \"B-MONEY\": \"B-money\", \"I-MONEY\": \"I-money\",\n",
        "  \"B-PERCENT\": \"B-percent\", \"I-PERCENT\": \"I-percent\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VxHgIvGX1z37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/30 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<unknown>, line 1)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3508\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[0;32mIn[10], line 30\u001b[0m\n    tag = ast.literal_eval(df.text.iloc[i])\u001b[0m\n",
            "\u001b[0m  File \u001b[0;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:64\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
            "\u001b[1;36m  File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
            "\u001b[1;36m  File \u001b[1;32m<unknown>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#https://spacy.io/usage/linguistic-features#native-tokenizer-additions\n",
        "##Testing token mismatches\n",
        "def custom_tokenizer(nlp):\n",
        "    infixes = (\n",
        "        LIST_ELLIPSES\n",
        "        + LIST_ICONS\n",
        "        + [\n",
        "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
        "            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
        "                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "            ),\n",
        "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    infix_re = compile_infix_regex(infixes)\n",
        "\n",
        "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
        "                                suffix_search=nlp.tokenizer.suffix_search,\n",
        "                                infix_finditer=infix_re.finditer,\n",
        "                                token_match=nlp.tokenizer.token_match,\n",
        "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "ner_list = []\n",
        "indices_to_drop = []\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "  tag = ast.literal_eval(df.Tag.iloc[i])\n",
        "  word = ast.literal_eval(df.Word.iloc[i])\n",
        "\n",
        "  sentence = ' '.join(word)\n",
        "\n",
        "  # Process the reconstructed sentence with spaCy\n",
        "  doc = nlp(sentence)\n",
        "  l=[]\n",
        "  for x in doc:\n",
        "    if x.ent_type_:\n",
        "      # l.append(f\"{x.ent_iob_}-{x.ent_type_.lower()[0:3]}\")\n",
        "      predicted_tag = f\"{x.ent_iob_}-{x.ent_type_}\"\n",
        "      l.append(tag_map[predicted_tag])\n",
        "    else:\n",
        "      l.append(f\"{x.ent_iob_}\")\n",
        "\n",
        "  if len(l) != len(tag):\n",
        "    indices_to_drop.append(i)\n",
        "  else:\n",
        "    ner_list.append(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygb02KnaUUTI"
      },
      "outputs": [],
      "source": [
        "# Custom function to parse columns based on the provided method\n",
        "def parse_column(entry):\n",
        "    # Split the entry to get the list elements\n",
        "    elements = entry.split(']')[0].split('[')[1].split(', ')\n",
        "    # Strip single and double quotes from each element\n",
        "    stripped_elements = [element.strip(\"'\\\"\") for element in elements]\n",
        "    return stripped_elements\n",
        "    # return entry.split(']')[0].split('[')[1].split(', ')\n",
        "\n",
        "# Apply the custom parsing function to each column (Word, POS, Tag)\n",
        "if type(df['Word'].iloc[0])!=type([]):\n",
        "  df['Word'] = df['Word'].apply(lambda x: parse_column(x))\n",
        "  df['POS'] = df['POS'].apply(lambda x: parse_column(x))\n",
        "  df['Tag'] = df['Tag'].apply(lambda x: parse_column(x))\n",
        "\n",
        "# Calculate sentence lengths\n",
        "df['Sentence_Length'] = df['Word'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M2N5Dm0UinY"
      },
      "outputs": [],
      "source": [
        "after_removing_df = df.drop(indices_to_drop).reset_index(drop=True)\n",
        "after_removing_df['ner_predicted'] = ner_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypW-AEkX2AIo"
      },
      "outputs": [],
      "source": [
        "# Frequency distribution of POS tags\n",
        "pos_tags = [pos for sublist in df['POS'] for pos in sublist]\n",
        "pos_tag_counts = Counter(pos_tags)\n",
        "\n",
        "# Frequency distribution of POS tags\n",
        "pos_tags_after = [pos for sublist in after_removing_df['POS'] for pos in sublist]\n",
        "pos_tag_counts_after = Counter(pos_tags_after)\n",
        "\n",
        "# Frequency distribution of NER tags\n",
        "ner_tags = [tag for sublist in df['Tag'] for tag in sublist]\n",
        "ner_tag_counts = Counter(ner_tags)\n",
        "\n",
        "# Frequency distribution of NER tags\n",
        "ner_tags_after = [tag for sublist in after_removing_df['Tag'] for tag in sublist]\n",
        "ner_tag_counts_after = Counter(ner_tags_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZvcKYfzOWIy"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of sentence lengths before removal with Plotly\n",
        "fig_before = px.histogram(df, x='Sentence_Length', nbins=30, title='Distribution of Sentence Lengths Before Removal')\n",
        "fig_before.update_layout(xaxis_title='Sentence Length', yaxis_title='Frequency', bargap=0.2)\n",
        "fig_before.show()\n",
        "\n",
        "# Plot the distribution of sentence lengths after removal with Plotly\n",
        "fig_after = px.histogram(after_removing_df, x='Sentence_Length', nbins=30, title='Distribution of Sentence Lengths After Removal')\n",
        "fig_after.update_layout(xaxis_title='Sentence Length', yaxis_title='Frequency', bargap=0.2)\n",
        "fig_after.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc5POo_SO9Tw"
      },
      "outputs": [],
      "source": [
        "# Assuming pos_tag_counts is available\n",
        "fig_pos = px.bar(x=list(pos_tag_counts.keys()), y=list(pos_tag_counts.values()), labels={'x': 'POS Tag', 'y': 'Frequency'}, title='Frequency Distribution of POS Tags')\n",
        "fig_pos.update_layout(xaxis_tickangle=-45)\n",
        "fig_pos.show()\n",
        "\n",
        "# For after removal, calculate POS tag counts again using after_removing_df\n",
        "# This assumes you've recalculated pos_tag_counts for the after_removing_df\n",
        "fig_pos_after = px.bar(x=list(pos_tag_counts_after.keys()), y=list(pos_tag_counts_after.values()), labels={'x': 'POS Tag', 'y': 'Frequency'}, title='Frequency Distribution of POS Tags After Removal')\n",
        "fig_pos_after.update_layout(xaxis_tickangle=-45)\n",
        "fig_pos_after.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVqGQQumPkoZ"
      },
      "outputs": [],
      "source": [
        "fig_ner = px.bar(x=list(ner_tag_counts.keys()), y=list(ner_tag_counts.values()), labels={'x': 'NER Tag', 'y': 'Frequency'}, title='Frequency Distribution of NER Tags')\n",
        "fig_ner.update_layout(xaxis_tickangle=-45)\n",
        "fig_ner.show()\n",
        "\n",
        "# For after removal, calculate NER tag counts again using after_removing_df\n",
        "# This assumes you've recalculated ner_tag_counts for the after_removing_df\n",
        "fig_ner_after = px.bar(x=list(ner_tag_counts_after.keys()), y=list(ner_tag_counts_after.values()), labels={'x': 'NER Tag', 'y': 'Frequency'}, title='Frequency Distribution of NER Tags After Removal')\n",
        "fig_ner_after.update_layout(xaxis_tickangle=-45)\n",
        "fig_ner_after.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QbLEKSvQEzH"
      },
      "outputs": [],
      "source": [
        "print(ner_tag_counts)\n",
        "print(ner_tag_counts_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLszksmNRfzU"
      },
      "outputs": [],
      "source": [
        "print(pos_tag_counts)\n",
        "print(pos_tag_counts_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXCumTz0NWwG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Plot the distribution of sentence lengths before removal\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Sentence_Length'], bins=30, kde=True)\n",
        "plt.title('Distribution of Sentence Lengths Before Removal')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the distribution of sentence lengths before removal\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(after_removing_df['Sentence_Length'], bins=30, kde=True)\n",
        "plt.title('Distribution of Sentence Lengths After Removal')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xe5GEYGqf_a"
      },
      "outputs": [],
      "source": [
        "y_true = [tag for sublist in after_removing_df['Tag'] for tag in sublist]\n",
        "y_pred = [tag for sublist in after_removing_df['ner_predicted'] for tag in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orEH3Bm2q6Yw"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(y_true,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g9OUqGO91_U"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5dihSNdq-D6"
      },
      "outputs": [],
      "source": [
        "# Identify all unique labels\n",
        "all_labels = sorted(set(y_true + y_pred))\n",
        "\n",
        "# Count the support for each label in y_true\n",
        "support_counts = {label: y_true.count(label) for label in all_labels}\n",
        "\n",
        "# Filter out labels with zero support\n",
        "labels_with_support = [label for label, count in support_counts.items() if count > 0]\n",
        "\n",
        "# Filter y_true and y_pred to include only instances with labels having non-zero support\n",
        "y_true_filtered = [y_true[i] for i in range(len(y_true)) if y_true[i] in labels_with_support]\n",
        "y_pred_filtered = [y_pred[i] for i in range(len(y_pred)) if y_true[i] in labels_with_support]\n",
        "\n",
        "# Generate the classification report for filtered lists\n",
        "report = classification_report(y_true_filtered, y_pred_filtered, labels=labels_with_support, zero_division=0)\n",
        "\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8FbYNGirSQL"
      },
      "outputs": [],
      "source": [
        "# PERSON:      People, including fictional.\n",
        "# NORP:        Nationalities or religious or political groups.\n",
        "# FAC:         Buildings, airports, highways, bridges, etc.\n",
        "# ORG:         Companies, agencies, institutions, etc.\n",
        "# GPE:         Countries, cities, states.\n",
        "# LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
        "# PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
        "# EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
        "# WORK_OF_ART: Titles of books, songs, etc.\n",
        "# LAW:         Named documents made into laws.\n",
        "# LANGUAGE:    Any named language.\n",
        "# DATE:        Absolute or relative dates or periods.\n",
        "# TIME:        Times smaller than a day.\n",
        "# PERCENT:     Percentage, including ”%“.\n",
        "# MONEY:       Monetary values, including unit.\n",
        "# QUANTITY:    Measurements, as of weight or distance.\n",
        "# ORDINAL:     “first”, “second”, etc.\n",
        "# CARDINAL:    Numerals that do not fall under another type.\n",
        "\n",
        "# geo = Geographical Entity\n",
        "# org = Organization\n",
        "# per = Person\n",
        "# gpe = Geopolitical Entity\n",
        "# tim = Time indicator\n",
        "# art = Artifact\n",
        "# eve = Event\n",
        "# nat = Natural Phenomenon\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5wHaKnnrIVl"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=labels_with_support)\n",
        "\n",
        "# Extract labels from the data\n",
        "labels = labels_with_support\n",
        "\n",
        "# Create a heatmap using Plotly Graph Objects\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=cm,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Confusion Matrix',\n",
        "    xaxis_title='Predicted Label',\n",
        "    yaxis_title='True Label',\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(labels))),\n",
        "        ticktext=labels\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=list(range(len(labels))),\n",
        "        ticktext=labels,\n",
        "        tickangle=-45  # Rotate labels\n",
        "    ),\n",
        "    autosize=False,\n",
        "    width=800,  # Adjust width\n",
        "    height=800,  # Adjust height to accommodate rotated labels\n",
        "    margin=dict(l=200, r=50, t=50, b=100)  # Adjust margins (left, right, top, bottom)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-FYHBaCbEtB"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from the confusion matrix and labels\n",
        "conf_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.set(font_scale=1.2)\n",
        "\n",
        "# Plot the heatmap with a custom color map\n",
        "heatmap = sns.heatmap(conf_df, annot=True, fmt='d', cmap=\"Greys\", cbar=False, linewidths=.5)\n",
        "\n",
        "# Set labels and title\n",
        "heatmap.set_xlabel('Predicted Labels', fontsize=16)\n",
        "heatmap.set_ylabel('Actual Labels', fontsize=16)\n",
        "heatmap.set_title('Confusion Matrix', fontsize=18)\n",
        "\n",
        "# Rotate the tick labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hbK18axjhu5"
      },
      "outputs": [],
      "source": [
        "nlp.get_pipe('ner').labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
